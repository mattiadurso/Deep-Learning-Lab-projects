{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ddoGuPFFKTOc"
   },
   "source": [
    "# An intro to PyTorch\n",
    "\n",
    "At its core, PyTorch provides two main features:\n",
    "\n",
    "- An n-dimensional Tensor, similar to numpy but can run on GPUs\n",
    "- Automatic differentiation for building and training neural networks\n",
    "\n",
    "We will use a fully-connected ReLU network as our running example. The network will have a single hidden layer, and will be trained with gradient descent to fit random data by minimizing the Euclidean distance between the network output and the true output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S_hXQPBhLo-d"
   },
   "source": [
    "**Scheme for the Architecture**\n",
    "\n",
    "```\n",
    "                   INPUT (x)                       LABELS (y)\n",
    "                 [ N x D_in ]                    [ N x D_out ]\n",
    "        +--------------------------+                   |\n",
    "         \\        Linear          /                    |\n",
    "          \\       Layer          /                     |\n",
    "           +--------------------+                      |\n",
    "                 [ N x H ]                             |\n",
    "           +--------------------+                      |\n",
    "           |       ReLu         |                      |\n",
    "           +--------------------+                      |\n",
    "                 [ N x H ]                             |\n",
    "           +--------------------+                      |\n",
    "          /       Linear         \\                     |\n",
    "         /        Layer           \\                    |\n",
    "        +--------------------------+                   |\n",
    "               [ N x D_out ]                           |\n",
    "             PREDICTION (y_pred)                       |\n",
    "                     |                                 |\n",
    "                     +----------->(loss_fn)<-----------+\n",
    "                                      |\n",
    "                                    LOSS\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hI-BOzyBo1TZ"
   },
   "source": [
    "We will first implement it in numpy and then take a look at all the features in PyTorch which make its implementation easier and more efficient.\n",
    "\n",
    "In the end you will be prompted to create a variation of this architecture and your own and train it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oiRNbu0WKa-Q"
   },
   "source": [
    "<a name=\"outline\"></a>\n",
    "## Outline\n",
    "- [1) Warm-up: numpy](#1)\n",
    "- [2) PyTorch: Tensors](#2)\n",
    "- [3) PyTorch: Autograd](#3)\n",
    "- [4) PyTorch: nn](#4)\n",
    "- [5) PyTorch: optim](#5)\n",
    "- [6) Custom nn Modules](#6)\n",
    "- [7) Control Flow + Weight Sharing [Exercise]](#7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fXLh8EOOKTOd"
   },
   "source": [
    "## 1) Warm-up: numpy\n",
    "<a name=\"1\"></a>\n",
    "[back to Outline](#outline)\n",
    "\n",
    "Before introducing PyTorch, we will first implement the neural network using numpy.\n",
    "\n",
    "Numpy provides an n-dimensional array object, and many functions for manipulating these arrays.\n",
    "\n",
    "Numpy is a generic framework for scientific computing; it does not know anything about computation graphs, or deep learning, or gradients.\n",
    "\n",
    "However we can easily use numpy to fit a two-layer network to random data by manually implementing the forward and backward passes through the network using numpy operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Oa-ZXRhNKTOe"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "loss_df = pd.DataFrame(index=list(range(500)), columns=[\"loss\"])\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random input and output data\n",
    "x = np.random.randn(N, D_in)\n",
    "y = np.random.randn(N, D_out)\n",
    "\n",
    "# Randomly initialize weights\n",
    "w1 = np.random.randn(D_in, H)\n",
    "w2 = np.random.randn(H, D_out)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for epoch in range(500):\n",
    "    # Forward pass: compute predicted y\n",
    "    h = x.dot(w1)\n",
    "    h_relu = np.maximum(h, 0)\n",
    "    y_pred = h_relu.dot(w2)\n",
    "\n",
    "    # Compute and store loss\n",
    "    loss = np.square(y_pred - y).sum()\n",
    "    loss_df.at[epoch, \"loss\"] = loss\n",
    "\n",
    "    # Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.T.dot(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.dot(w2.T)\n",
    "    grad_h = grad_h_relu.copy()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.T.dot(grad_h)\n",
    "\n",
    "    # Update weights\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QVQfOWOWKTOj"
   },
   "source": [
    "Quick reminder: we can use `pandas` to quickly visualize the values of the loss during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "eBPsBXD_KTOl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            loss\n",
      "1    2.99389e+07\n",
      "2    3.01373e+07\n",
      "3    2.92171e+07\n",
      "497  1.02335e-07\n",
      "498  9.69849e-08\n",
      "499  9.19156e-08\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# print first three and last three values of the loss\n",
    "print(loss_df.iloc[[1,2,3, -3,-2,-1]])\n",
    "\n",
    "xs = loss_df.plot()\n",
    "_ = xs.set_ylabel(\"loss\")\n",
    "_ = xs.set_xlabel(\"epoch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HdMnqFOGKTOu"
   },
   "source": [
    "## 2) PyTorch: Tensors\n",
    "<a name=\"2\"></a>\n",
    "[back to Outline](#outline)\n",
    "\n",
    "As you saw numpy is a great framework and we can implement a neural network with it... but it cannot utilize **GPUs** to accelerate its numerical computations. For modern deep neural networks, GPUs often provide speedups of **50x or greater**, so unfortunately numpy won't be enough for modern deep learning.\n",
    "\n",
    "Here we introduce the most fundamental PyTorch concept: the **Tensor**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6rXkVKDLV_Wr"
   },
   "source": [
    "### Creating Tensors\n",
    "\n",
    "A PyTorch Tensor is conceptually identical to a numpy array: a Tensor is an n-dimensional array, and PyTorch provides many functions for operating on these Tensors.\n",
    "\n",
    "_Any computation you might want to perform with numpy can also be accomplished with PyTorch Tensors_. You should think of them as a generic tool for scientific computing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "IrLZTCxUKTOv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000e+00, 1.4013e-45, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [1.1614e-41, 0.0000e+00, 2.2369e+08],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Here is a 5x3 matrix, uninitialized:\n",
    "# it does not contain definite known values before it is used\n",
    "\n",
    "x = torch.empty(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ulymRv1LKTOz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3543, 0.5612, 0.6486],\n",
      "        [0.4240, 0.6980, 0.9975],\n",
      "        [0.6176, 0.1099, 0.2249],\n",
      "        [0.5379, 0.4828, 0.5170],\n",
      "        [0.3960, 0.6877, 0.7253]])\n"
     ]
    }
   ],
   "source": [
    "# Construct a randomly initialized matrix\n",
    "\n",
    "x = torch.rand(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "8c-r8eKwKTO3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "# Construct a matrix filled zeros and of dtype long:\n",
    "\n",
    "x = torch.zeros(5, 3, dtype=torch.long)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "-WKjyPR7KTO6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# Create an array of all ones\n",
    "b = torch.ones((1,2))   \n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "ObE11IMEKTO-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5.5000, 3.0000])\n"
     ]
    }
   ],
   "source": [
    "# Construct a tensor directly from data:\n",
    "\n",
    "x = torch.tensor([5.5, 3])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "1BWDboZAKTPB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5.5000, 3.0000])\n",
      "tensor([1., 1.])\n"
     ]
    }
   ],
   "source": [
    "# you can use *_like(t) to create tensors of zeros/ones/random with the same shape of t\n",
    "\n",
    "b = torch.ones_like(x)\n",
    "print(x)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "g9oh5Vu-KTPE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]])\n",
      "<class 'numpy.ndarray'>\n",
      "[[0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]]\n",
      "\n",
      "<class 'torch.Tensor'>\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.]])\n",
      "<class 'numpy.ndarray'>\n",
      "[[1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# torch tensors can be converted into numpy arrays\n",
    "# and keep them connected\n",
    "\n",
    "x = torch.zeros(3,2)\n",
    "print(type(x)); print(x)\n",
    "\n",
    "y = x.numpy()\n",
    "print(type(y)); print(y)\n",
    "\n",
    "\n",
    "print()\n",
    "\n",
    "x += 1\n",
    "print(type(x)); print(x)\n",
    "print(type(y)); print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "I7-Bu_F6KTPG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.]])\n",
      "\n",
      "[[2. 2.]\n",
      " [2. 2.]\n",
      " [2. 2.]]\n",
      "tensor([[2., 2.],\n",
      "        [2., 2.],\n",
      "        [2., 2.]])\n"
     ]
    }
   ],
   "source": [
    "# the same can be done the other way\n",
    "\n",
    "z = torch.from_numpy(y)\n",
    "print(z)\n",
    "print()\n",
    "\n",
    "y+=1\n",
    "\n",
    "print(y)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "7wus1_9RKTPJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1417,  0.4522, -0.2078,  0.3465],\n",
      "        [ 1.1520,  0.3241,  1.5589, -0.3883],\n",
      "        [-2.8291, -0.2369, -0.4588,  0.6151],\n",
      "        [-0.6376,  1.7853,  0.7089, -1.4544]])\n",
      "torch.Size([4, 4]) \n",
      "\n",
      "\n",
      "tensor([ 0.1417,  0.4522, -0.2078,  0.3465,  1.1520,  0.3241,  1.5589, -0.3883,\n",
      "        -2.8291, -0.2369, -0.4588,  0.6151, -0.6376,  1.7853,  0.7089, -1.4544])\n",
      "torch.Size([16]) \n",
      "\n",
      "\n",
      "tensor([[ 0.1417,  0.4522, -0.2078,  0.3465,  1.1520,  0.3241,  1.5589, -0.3883],\n",
      "        [-2.8291, -0.2369, -0.4588,  0.6151, -0.6376,  1.7853,  0.7089, -1.4544]])\n",
      "torch.Size([2, 8])\n",
      "torch.Size([2, 8]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# each tensor has a .size() (or .shape) and can be reshaped\n",
    "\n",
    "x = torch.randn(4, 4)\n",
    "y = x.view(16)\n",
    "z = x.view(-1, 8)  # the size -1 is inferred from other dimensions\n",
    "\n",
    "print(x)\n",
    "print(x.size(), \"\\n\")\n",
    "\n",
    "print()\n",
    "\n",
    "print(y)\n",
    "print(y.size(), \"\\n\")\n",
    "\n",
    "print()\n",
    "\n",
    "print(z)\n",
    "print(z.size())\n",
    "print(z.shape, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wdt13e4CKTPM"
   },
   "source": [
    "### Tensors on the GPU\n",
    "\n",
    "However unlike numpy, PyTorch Tensors can utilize GPUs to accelerate their numeric computations. To run a PyTorch Tensor on GPU, you use the device argument when constructing a Tensor to place the Tensor on a GPU.\n",
    "\n",
    "Tensors can be moved onto any device using the `.to` method, or created directly onto any device using `device`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "_68w3xtlKTPN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda not available\n"
     ]
    }
   ],
   "source": [
    "# We will use torch.device objects to move tensors in and out of GPU\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Cuda available\")\n",
    "    device = torch.device(\"cuda\")          # a CUDA device object\n",
    "    y = torch.ones_like(x, device=device)  # directly create a tensor on GPU\n",
    "    x = x.to(device)                       # or just use strings .to(\"cuda\")\n",
    "    z = x + y\n",
    "    print(z)\n",
    "    print(z.to(\"cpu\", torch.double))       # .to can also change dtype together!\n",
    "else:\n",
    "    print(\"Cuda not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FDfi1b_vKTPQ"
   },
   "source": [
    "Here we use PyTorch Tensors to fit a two-layer network to random data. Like the numpy example above we manually implement the forward and backward passes through the network, using operations on PyTorch Tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "ORKcPctqKTPR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            loss\n",
      "1    2.81198e+07\n",
      "2    2.48444e+07\n",
      "3    2.03242e+07\n",
      "497  3.09069e-05\n",
      "498  3.05787e-05\n",
      "499  3.01044e-05\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "loss_df = pd.DataFrame(index=list(range(500)), columns=[\"loss\"])\n",
    "\n",
    "device = torch.device('cpu')\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda') # Uncomment this to run on GPU\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random input and output data as tensors\n",
    "# directly on the currente device\n",
    "x =  torch.randn(N, D_in, device=device) # [BLANK]\n",
    "y = torch.randn(N, D_out, device=device) # [BLANK]\n",
    "\n",
    "# Randomly initialize weights as tensors\n",
    "w1 = torch.randn(D_in, H, device=device)  # [BLANK]\n",
    "w2 = torch.randn(H, D_out, device=device) # [BLANK]\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for epoch in range(500):\n",
    "    # Forward pass: compute predicted y\n",
    "    h = x.mm(w1)\n",
    "    h_relu = h.clamp(min=0)\n",
    "    y_pred = h_relu.mm(w2)\n",
    "\n",
    "    # Compute and store loss; loss is a scalar, and is stored in a PyTorch Tensor\n",
    "    # of shape (); we can get its value as a Python number with loss.item().\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    loss_df.at[epoch, \"loss\"] = loss.item()\n",
    "\n",
    "    # Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "    grad_h = grad_h_relu.clone()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.t().mm(grad_h)\n",
    "\n",
    "    # Update weights using gradient descent\n",
    "    w1 -= learning_rate * grad_w1  # [BLANK]\n",
    "    w2 -= learning_rate * grad_w2  # [BLANK]\n",
    "\n",
    "# print first three and last three values of the loss\n",
    "print(loss_df.iloc[[1,2,3, -3,-2,-1]])\n",
    "\n",
    "xs = loss_df.plot()\n",
    "_ = xs.set_ylabel(\"loss\")\n",
    "_ = xs.set_xlabel(\"epoch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iICyMpjwKTPU"
   },
   "source": [
    "## 3) PyTorch: Autograd\n",
    "<a name=\"3\"></a>\n",
    "[back to Outline](#outline)\n",
    "\n",
    "In the above examples, we had to manually implement both the forward and backward passes of our neural network. Manually implementing the backward pass is not a big deal for a small two-layer network, but can quickly get very hairy for large complex networks.\n",
    "\n",
    "Thankfully, we can use **automatic differentiation** to automate the computation of backward passes in neural networks.\n",
    "\n",
    "The **autograd** package in PyTorch provides exactly this functionality. When using autograd, the forward pass of your network will define a **computational graph**.\n",
    "\n",
    "Nodes in the graph will be Tensors, and edges will be functions that produce output Tensors from input Tensors. Backpropagating through this graph then allows you to easily compute gradients.\n",
    "\n",
    "This sounds complicated, it's pretty simple to use in practice. If we want to compute gradients with respect to some Tensor, then we set `requires_grad=True` when constructing that Tensor.\n",
    "\n",
    "Any PyTorch operations on that Tensor will cause a computational graph to be constructed, allowing us to later perform backpropagation through the graph. If `x` is a Tensor with `requires_grad=True`, then after backpropagation `x.grad` will be another Tensor holding the gradient of `x` with respect to some scalar value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JOFGfA63VDw9"
   },
   "source": [
    "### requires_grad and gradients\n",
    "\n",
    "Let's create a tensor and set `requires_grad=True` to track computation with it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "RPIDHiM6KTPV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(1, 1, requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PHvZocSxU8Cm"
   },
   "source": [
    "Let's do a tensor operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "mk-nUliUKTPZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3.]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y = x + 2\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oe4Khs58KTPc"
   },
   "source": [
    "`y` was created as a result of an operation on a tensor which \"required grad\". So it has a `grad_fn`, and it requires grad too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "nzF6ArubKTPc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<AddBackward0 object at 0x127557dd0>\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(y.grad_fn)\n",
    "print(y.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "M_5UNxicKTPf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[27.]], grad_fn=<PowBackward0>)\n"
     ]
    }
   ],
   "source": [
    "out = y ** 3\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a-P8kpIJKTPk"
   },
   "source": [
    "Letâ€™s backprop now using `out.backward()` and check the gradients of the first input `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "eYtw4nHjKTPm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[27.]])\n"
     ]
    }
   ],
   "source": [
    "out.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-e4KD4o4KTPp"
   },
   "source": [
    "If we were to calculate the gradients by hand, we would have:\n",
    "\n",
    "$out = (x+2)^3$\n",
    "\n",
    "$\\frac{\\partial out}{\\partial x} = 3(x+2)^2$\n",
    "\n",
    "$\\frac{\\partial out}{\\partial x}\\bigr\\rvert_{x=1} = 3(1+2)^2 = 27$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ISv3Q2_UKTPp"
   },
   "source": [
    "### Resetting the gradient\n",
    "\n",
    "Pay attention to the fact that **gradientes are not reset automatically** in pytorch, but they keep adding up.\n",
    "This is useful in some architectures.\n",
    "\n",
    "Notice the difference in the following three cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "PVeR0eyWKTPq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.], requires_grad=True)\n",
      "4.0 gradient of x after 1 operation(s)\n",
      "8.0 gradient of x after 2 operation(s)\n",
      "12.0 gradient of x after 3 operation(s)\n",
      "-------------\n",
      "value of y.\n",
      "4.0\n",
      "gradient of x^2 accumulated 3 times.\n",
      "12.0\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([2.], requires_grad=True)\n",
    "print(x)\n",
    "\n",
    "for i in range(3):\n",
    "    y = x**2\n",
    "    y.backward()\n",
    "    print(x.grad.item(), f\"gradient of x after {i+1} operation(s)\")\n",
    "print(\"-------------\")\n",
    "print(\"value of y.\")\n",
    "print(y.item())\n",
    "print(\"gradient of x^2 accumulated 3 times.\")\n",
    "print(x.grad.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "ze7J1Q40KTPs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.], requires_grad=True)\n",
      "-------------\n",
      "value of y.\n",
      "4.0\n",
      "gradient of x^2.\n",
      "4.0\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([2.], requires_grad=True)\n",
    "print(x)\n",
    "\n",
    "for i in range(3):\n",
    "    y = x**2\n",
    "y.backward()\n",
    "print(\"-------------\")\n",
    "print(\"value of y.\")\n",
    "print(y.item())\n",
    "print(\"gradient of x^2.\")\n",
    "print(x.grad.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "-W5IyaXmKTPx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.], requires_grad=True)\n",
      "4.0 gradient of x after 1 operation(s)\n",
      "4.0 gradient of x after 2 operation(s)\n",
      "4.0 gradient of x after 3 operation(s)\n",
      "-------------\n",
      "value of y.\n",
      "4.0\n",
      "gradient of x^2.\n",
      "4.0\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([2.], requires_grad=True)\n",
    "print(x)\n",
    "\n",
    "for i in range(3):\n",
    "    if x.grad:  # manually zeroing the gradient\n",
    "        x.grad.zero_()\n",
    "    y = x**2\n",
    "    y.backward()\n",
    "    print(x.grad.item(), f\"gradient of x after {i+1} operation(s)\")\n",
    "    \n",
    "print(\"-------------\")\n",
    "print(\"value of y.\")\n",
    "print(y.item())\n",
    "print(\"gradient of x^2.\")\n",
    "print(x.grad.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OUQ8ZkG0KTP3"
   },
   "source": [
    "### Stop tracking the gradients\n",
    "\n",
    "Sometimes you may wish to _prevent PyTorch from building computational graphs_ when performing certain operations on Tensors with `requires_grad=True`. For example we usually don't want to backpropagate through the weight update steps when **testing** a neural network.\n",
    "\n",
    "In such scenarios we can use the `torch.no_grad()` context manager to prevent the construction of a computational graph.\n",
    "\n",
    "Or use `.detach()` to get a new Tensor with the same content but that does not require gradients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "_2LzCI9wKTP4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(2,2,requires_grad=True)\n",
    "print(x.requires_grad)\n",
    "print((x ** 2).requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    print((x ** 2).requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "xOlOuBV1KTP7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "tensor(True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(2,2,requires_grad=True)\n",
    "print(x.requires_grad)\n",
    "y = x.detach()\n",
    "print(y.requires_grad)\n",
    "print(x.eq(y).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E5kbPEyrKTP-"
   },
   "source": [
    "### Let's make the point\n",
    "\n",
    "This means that the lines of code that we previously used to manually compute the gradients...\n",
    "\n",
    "```python\n",
    "...\n",
    "    loss = [CODE TO CALCULATE LOSS]\n",
    "\n",
    "    # Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "    grad_h = grad_h_relu.clone()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.t().mm(grad_h)\n",
    "...\n",
    "```\n",
    "\n",
    "... are no longer required, as the gradients are computed automatically by autograd and stored in the tensors themselves.\n",
    "\n",
    "We just need to **reset the gradients first, and then call the `.backward()` function** at each epoch.\n",
    "\n",
    "```python\n",
    "...\n",
    "    loss = [CODE TO CALCULATE LOSS]\n",
    "    \n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jazyAClEKTP_"
   },
   "source": [
    "## 4) PyTorch: nn\n",
    "<a name=\"4\"></a>\n",
    "[back to Outline](#outline)\n",
    "\n",
    "Computational graphs and autograd are a very powerful paradigm for defining complex operators and automatically taking derivatives; however for large neural networks raw autograd can be a bit too low-level.\n",
    "\n",
    "When building neural networks we frequently think of arranging the computation into **layers**, some of which have **learnable parameters** which will be **optimized** during learning.\n",
    "\n",
    "In PyTorch, the **`nn` package** defines a set of Modules, which are roughly equivalent to neural network layers.\n",
    "A Module receives input Tensors and computes output Tensors, but may also hold internal state such as Tensors containing learnable parameters.\n",
    "\n",
    "The `nn` package also defines a set of useful **loss functions** that are commonly used when training neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y6yxjm5tKTQA"
   },
   "source": [
    "### A Sequential model\n",
    "\n",
    "The modules we need to create the same neural network as before are:\n",
    "\n",
    "- `torch.nn.Linear(input_size, hidden_size)` (used twice)\n",
    "- `torch.nn.ReLu()`\n",
    "\n",
    "Layers can be concatenated to create a sequential model with little to no effort using\n",
    "\n",
    "`model = torch.nn.Sequential(layer_1, layer_2, ..., layer_n)`\n",
    "\n",
    "The loss function we used up until can be created using:\n",
    "\n",
    "- `torch.nn.MSELoss(reduction='sum')`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tfKGQ9DuKTQA"
   },
   "source": [
    "Try and create:\n",
    "\n",
    "- a simple sequential model with the same architecture as the previous ones\n",
    "- a loss function as suggested in the explanation above\n",
    "- use the model to get predictions for an input `x` (a `N x D_in` matrix of ones)\n",
    "- use the loss function to compute the differenct between `preds` and `y` (loss functions take two arguments, in this order)\n",
    "\n",
    "Compare your results with the expected output below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "W10mwXHFKTQA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=1000, out_features=100, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=100, out_features=5, bias=True)\n",
      ")\n",
      "MSELoss()\n",
      "tensor([[ 0.1848,  0.1407,  0.0075, -0.5831,  0.3746],\n",
      "        [ 0.1848,  0.1407,  0.0075, -0.5831,  0.3746],\n",
      "        [ 0.1848,  0.1407,  0.0075, -0.5831,  0.3746],\n",
      "        [ 0.1848,  0.1407,  0.0075, -0.5831,  0.3746],\n",
      "        [ 0.1848,  0.1407,  0.0075, -0.5831,  0.3746],\n",
      "        [ 0.1848,  0.1407,  0.0075, -0.5831,  0.3746],\n",
      "        [ 0.1848,  0.1407,  0.0075, -0.5831,  0.3746],\n",
      "        [ 0.1848,  0.1407,  0.0075, -0.5831,  0.3746],\n",
      "        [ 0.1848,  0.1407,  0.0075, -0.5831,  0.3746],\n",
      "        [ 0.1848,  0.1407,  0.0075, -0.5831,  0.3746]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "tensor(52.8523, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# fix random seed for reproducibility (hopefully)\n",
    "torch.manual_seed(123)\n",
    "torch.cuda.manual_seed(123)\n",
    "np.random.seed(123)\n",
    "random.seed(123)\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N = 10 \n",
    "D_in = 1000\n",
    "H = 100\n",
    "D_out = 5\n",
    "\n",
    "# Create Tensors of ONES to hold inputs and outputs\n",
    "x = torch.ones(N, D_in)\n",
    "y = torch.ones(N, D_out)\n",
    "\n",
    "# Create a Sequential model with the same architecture\n",
    "# as the previous ones (see image at the beginning of the notebook)\n",
    "# Move it to the current device using .to(device)\n",
    "# as you would to with a Tensor\n",
    "model = torch.nn.Sequential(\n",
    "        nn.Linear(D_in,H), \n",
    "        nn.ReLU(), \n",
    "        nn.Linear(H,D_out)).to(device)  # [BLANK]\n",
    "\n",
    "# Create the loss function\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')  # [BLANK]\n",
    "# Get the predictions from the model:\n",
    "# just use it as a function, passing x as the input\n",
    "preds = model(x)  # [BLANK]\n",
    "# Calculate the loss\n",
    "loss = loss_fn(preds, y)  # [BLANK]\n",
    "\n",
    "print(model)\n",
    "print(loss_fn)\n",
    "print(preds)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-GpxeGUiKTQE"
   },
   "source": [
    "**Expected output:**\n",
    "\n",
    "```python\n",
    "Sequential(\n",
    "  (0): Linear(in_features=1000, out_features=100, bias=True)\n",
    "  (1): ReLU()\n",
    "  (2): Linear(in_features=100, out_features=5, bias=True)\n",
    ")\n",
    "MSELoss()\n",
    "tensor([[ 0.1848,  0.1407,  0.0075, -0.5831,  0.3746],\n",
    "        [ 0.1848,  0.1407,  0.0075, -0.5831,  0.3746],\n",
    "        [ 0.1848,  0.1407,  0.0075, -0.5831,  0.3746],\n",
    "        [ 0.1848,  0.1407,  0.0075, -0.5831,  0.3746],\n",
    "        [ 0.1848,  0.1407,  0.0075, -0.5831,  0.3746],\n",
    "        [ 0.1848,  0.1407,  0.0075, -0.5831,  0.3746],\n",
    "        [ 0.1848,  0.1407,  0.0075, -0.5831,  0.3746],\n",
    "        [ 0.1848,  0.1407,  0.0075, -0.5831,  0.3746],\n",
    "        [ 0.1848,  0.1407,  0.0075, -0.5831,  0.3746],\n",
    "        [ 0.1848,  0.1407,  0.0075, -0.5831,  0.3746]],\n",
    "       grad_fn=<AddmmBackward>)\n",
    "tensor(52.8523, grad_fn=<MseLossBackward>)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wuPYj-o8KTQF"
   },
   "source": [
    "## 5) PyTorch: optim\n",
    "<a name=\"5\"></a>\n",
    "[back to Outline](#outline)\n",
    "\n",
    "Up to this point we have updated the weights of our models by manually mutating Tensors holding learnable parameters. This is not a huge burden for simple optimization algorithms like stochastic gradient descent, but in practice we often train neural networks using more sophisiticated optimizers like AdaGrad, RMSProp, Adam, etc.\n",
    "\n",
    "The **`optim` package** in PyTorch abstracts the idea of an optimization algorithm and provides implementations of commonly used optimization algorithms.\n",
    "\n",
    "In this example we will use the `nn` package to define our model as before, but we will optimize the model using the Adam algorithm provided by the `optim` package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "Ie_Ez6aQKTQF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          loss\n",
      "1      199.422\n",
      "2       177.67\n",
      "3      157.561\n",
      "497  0.0633041\n",
      "498  0.0621613\n",
      "499  0.0610339\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "\n",
    "loss_df = pd.DataFrame(index=list(range(500)), columns=[\"loss\"])\n",
    "\n",
    "device = torch.device('cpu')\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda') # Uncomment this to run on GPU\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create RANDOM Tensors to hold inputs and outputs.\n",
    "x = torch.rand(N, D_in)  # [BLANK]\n",
    "y = torch.rand(N, D_out)  # [BLANK]\n",
    "\n",
    "# Use the nn package to define our model (Sequential) and loss function\n",
    "model = model = torch.nn.Sequential(\n",
    "        nn.Linear(D_in,H), \n",
    "        nn.ReLU(), \n",
    "        nn.Linear(H,D_out)).to(device)\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')  # [BLANK]\n",
    "\n",
    "\n",
    "# Use the optim package to define an Optimizer that will update the weights of\n",
    "# the model for us. Here we will use Adam; the optim package contains many other\n",
    "# optimization algorithms. The first argument to the Adam constructor tells the\n",
    "# optimizer which Tensors it should update.\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "for epoch in range(500):\n",
    "    # Forward pass: compute predicted y by passing x to the model.\n",
    "    y_pred = model(x)  # [BLANK]\n",
    "\n",
    "    # Compute and print loss.\n",
    "    loss = loss_fn(y_pred, y)  # [BLANK]\n",
    "    loss_df.at[epoch, \"loss\"] = loss.item()\n",
    "\n",
    "    # Before the backward pass, use the optimizer object to zero all of the\n",
    "    # gradients for the Tensors it will update (which are the learnable weights\n",
    "    # of the model)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Backward pass: compute gradient of the loss with respect to model parameters\n",
    "    loss.backward()  # [BLANK]\n",
    "\n",
    "    # Calling the step function on an Optimizer makes an update to its parameters\n",
    "    optimizer.step()\n",
    "    \n",
    "# print first three and last three values of the loss\n",
    "print(loss_df.iloc[[1,2,3, -3,-2,-1]])\n",
    "\n",
    "xs = loss_df.plot()\n",
    "_ = xs.set_ylabel(\"loss\")\n",
    "_ = xs.set_xlabel(\"epoch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rbUZa2NAKTQH"
   },
   "source": [
    "## 6) Custom nn Modules\n",
    "<a name=\"6\"></a>\n",
    "[back to Outline](#outline)\n",
    "\n",
    "Sometimes you will want to specify models that are more **complex than a sequence of existing Modules**; for these cases you can define your own Modules by subclassing `nn.Module` and defining a `forward` which receives input Tensors and produces output Tensors using other modules or other autograd operations on Tensors.\n",
    "\n",
    "In this example we implement our two-layer network as a custom Module subclass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "n46JCeZ3KTQI"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TwoLayerNet(\n",
       "  (linear1): Linear(in_features=1000, out_features=100, bias=True)\n",
       "  (linear2): Linear(in_features=100, out_features=10, bias=True)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "\n",
    "class TwoLayerNet(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate two nn.Linear modules and assign them as\n",
    "        member variables.\n",
    "        \"\"\"\n",
    "        super(TwoLayerNet, self).__init__()\n",
    "        self.linear1 = nn.Linear(D_in, H)  # [BLANK]\n",
    "        self.linear2 = nn.Linear(H, D_out)  # [BLANK]\n",
    "        self.relu = nn.ReLU()  # [BLANK]\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Tensor of input data and we must return\n",
    "        a Tensor of output data. We can use Modules defined in the constructor as\n",
    "        well as arbitrary (differentiable) operations on Tensors.\n",
    "        \"\"\"\n",
    "        hidden1 = self.linear1(x)\n",
    "        h_relu  = self.relu(hidden1)  # [BLANK]\n",
    "        y_pred  = self.linear2(h_relu)  # [BLANK]\n",
    "        return y_pred\n",
    "\n",
    "# Construct our model by instantiating the class defined above.\n",
    "model = TwoLayerNet(D_in, H, D_out)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8uzncKvLceDg"
   },
   "source": [
    "**Expected output:**\n",
    "\n",
    "```python\n",
    "TwoLayerNet(\n",
    "  (linear1): Linear(in_features=1000, out_features=100, bias=True)\n",
    "  (linear2): Linear(in_features=100, out_features=10, bias=True)\n",
    "  (relu): ReLU()\n",
    ")\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "_L_r0mwOXO2V"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            loss\n",
      "1        643.004\n",
      "2        597.683\n",
      "3        558.595\n",
      "497  5.87379e-06\n",
      "498  5.70722e-06\n",
      "499  5.54507e-06\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_df = pd.DataFrame(index=list(range(500)), columns=[\"loss\"])\n",
    "\n",
    "device = torch.device('cpu')\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda') # Uncomment this to run on GPU\n",
    "    \n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold inputs and outputs\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "# Construct our loss function and an Optimizer. The call to model.parameters()\n",
    "# in the SGD constructor will contain the learnable parameters of the two\n",
    "# nn.Linear modules which are members of the model.\n",
    "learning_rate = 1e-4\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')  # [BLANK]\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)   # use the optim.SGD optimizer this time!\n",
    "\n",
    "for epoch in range(500):\n",
    "\n",
    "    y_pred = model(x)  # [BLANK]  # Forward pass: Compute predicted y by passing x to the model\n",
    "    loss = loss_fn(y_pred, y)  # [BLANK]    # Compute and print loss\n",
    "    \n",
    "    loss_df.at[epoch, \"loss\"] = loss.item()\n",
    "\n",
    "    optimizer.zero_grad()  # [BLANK]  # Zero gradients, \n",
    "    loss.backward()  # [BLANK]  # perform a backward pass,\n",
    "    optimizer.step()  # [BLANK]  # and update the weights.\n",
    "    \n",
    "\n",
    "print(loss_df.iloc[[1,2,3, -3,-2,-1]])\n",
    "xs = loss_df.plot()\n",
    "_ = xs.set_ylabel(\"loss\")\n",
    "_ = xs.set_xlabel(\"epoch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Op1xpV-8KTQK"
   },
   "source": [
    "## 7) Control Flow + Weight Sharing [Exercise]\n",
    "<a name=\"7\"></a>\n",
    "[back to Outline](#outline)\n",
    "\n",
    "As an example of dynamic graphs and weight sharing, we implement a very strange model: a fully-connected ReLU network that on each forward pass chooses a random number between 1 and 4... and uses that many hidden layers! The hidden layers reuse the same weights multiple times to compute the innermost hidden representation.\n",
    "\n",
    "For this model you can use normal Python `for` loop and you can implement weight sharing among the innermost layers by simply reusing the same Module multiple times when defining the `forward` pass.\n",
    "\n",
    "Try it for yourself!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k-dqX-wHny-S"
   },
   "source": [
    "**Scheme for the Architecture**\n",
    "\n",
    "It only needs 4 modules:\n",
    "- an `input_linear` layer to generate the first hidden representation\n",
    "- a `middle_linear` layer (which will be reused)\n",
    "- an `output_linear` layer to output the final prediction\n",
    "- a `relu` layer for all the needed activations\n",
    "\n",
    "```python\n",
    "                   INPUT (x)\n",
    "                 [ N x D_in ]\n",
    "        +--------------------------+\n",
    "         \\      Linear Layer      /\n",
    "          \\    \"input_linear\"    /\n",
    "           +-----[ N x H ]------+\n",
    "           |       ReLu         |\n",
    "           +--------------------+\n",
    "                 [ N x H ]\n",
    "       . . . . . . . . . . . . . . . .\n",
    "       .   +--------------------+    . this block is\n",
    "       .   |    Linear Layer    |    . randomly repeated\n",
    "       .   |   \"middle_linear\"  |    . from 0 to 3 times\n",
    "       .   +-----[ N x H ]------+    . to generate\n",
    "       .   |       ReLu         |    . from 1 to 4\n",
    "       .   +--------------------+    . hidden layers\n",
    "       . . . . . . . . . . . . . . . .\n",
    "                 [ N x H ]\n",
    "           +--------------------+    \n",
    "          /     Linear Layer     \\   \n",
    "         /     \"ouput_linear\"     \\      \n",
    "        +--------------------------+ \n",
    "               [ N x D_out ]      \n",
    "             PREDICTION (y_pred)  \n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "5RO5zAlFoJSK"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "\n",
    "class DynamicNet(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        super(DynamicNet,self).__init__() \n",
    "        \n",
    "        self.layers = nn.Sequential( \n",
    "            nn.Linear(D_in,H), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(H,D_out))\n",
    "\n",
    "    def forward(self, x):\n",
    "        y_pred = self.layers(x)  # [BLANK]\n",
    "        return y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "EEg950s3KTQK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            loss\n",
      "1        632.766\n",
      "2        550.182\n",
      "3        459.166\n",
      "497  2.47315e-12\n",
      "498  2.36447e-12\n",
      "499  2.40911e-12\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_df = pd.DataFrame(index=list(range(500)), columns=[\"loss\"])\n",
    "\n",
    "device = torch.device('cpu')\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda') # Uncomment this to run on GPU\n",
    "    \n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold inputs and outputs.\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "# Construct our model by instantiating the class defined above\n",
    "model = DynamicNet(D_in, H, D_out)  # [BLANK]\n",
    "\n",
    "# Construct our loss function and an Optimizer. Training this strange model with\n",
    "# vanilla stochastic gradient descent is tough, so we use momentum\n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4, momentum=0.9)\n",
    "\n",
    "for epoch in range(500): # train your model!\n",
    "    y_pred = model(x)  # [BLANK]  # Forward pass: Compute predicted y by passing x to the model\n",
    "    loss = loss_fn(y_pred, y)  # [BLANK]    # Compute and print loss\n",
    "    loss_df.at[epoch, \"loss\"] = loss.item()\n",
    "\n",
    "    optimizer.zero_grad()  # [BLANK]  # Zero gradients, \n",
    "    loss.backward()  # [BLANK]  # perform a backward pass,\n",
    "    optimizer.step()  # [BLANK]\n",
    "\n",
    "# print first three and last three values of the loss\n",
    "print(loss_df.iloc[[1,2,3, -3,-2,-1]])\n",
    "\n",
    "xs = loss_df.plot()\n",
    "_ = xs.set_ylabel(\"loss\")\n",
    "_ = xs.set_xlabel(\"epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "04 - Intro to pytorch.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
