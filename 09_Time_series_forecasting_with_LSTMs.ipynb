{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZNeqlC3fqEFp"
   },
   "source": [
    "# Introduction to LSTMs\n",
    "Long Short-Term Memory networks, usually abbreviated in LSTM, are a kind of Recurrent Neural Network proposed by Hochreiter & Schmidhuber in their 1997 article [\"Long Short-Term Memory\"](http://www.bioinf.jku.at/publications/older/2604.pdf).\n",
    "\n",
    "LSTMs were designed in order to remember the data given in input for long periods of time, while avoiding issues afflicting other types of RNNs like the *vanishing gradient*, the *exploding gradient* the *long-term dependancy* problems.\n",
    "\n",
    "The *vanishing gradient* problem consists in the fact that RNNs, by repeatedly calculating the value of a gradient used to update the weights of the model, tend to make the value of such gradient converge to zero, thus preventing the network to learn effectively from the input data.\n",
    "\n",
    "The *exploding gradient* problem is similar to the vanishing gradient one, but in this case the value of the gradient tends to be updated to progressively bigger values, thus making the value of the gradient so high that the model becomes unstable and unable to learn from the training data.\n",
    "\n",
    "The *long-term dependency* problem consists in the inability of detecting associations between data points which are distant from each other in the input sequence.\n",
    "\n",
    "LSTM networks address the aforementioned problems by exploiting in each of its units (also called *cells*), an *input gate*, an *output gate* and a *forget gate*. While the cell remembers values over arbitrary time intervals, the three gates regulate the flow of information inside the cell and which information forward to the other cells.\n",
    "\n",
    "At the time **t** a LSTM cell takes in input the hidden layer **h** and the cell state **c** calculated by the LSTM at time **t-1** and the t-th portion of the input sequence **x**. The forget gate enstablishes which parts of the previous cell and hidden state to use in the new cell state, the input gate decides the extent to which a new value of the input sequence flows into the new cell state, and the ouput gate determines how to calculate the new hidden state of the LSTM cell (which will also be used to calculate the output **y** of the cell.\n",
    "\n",
    "![image](https://drive.google.com/uc?id=104-iQvZTMBMjilbJHSANFoMrM8_hMFil)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sTAO1yk_qH87"
   },
   "source": [
    "# Time series\n",
    "In this tutorial we will use LSTM networks in order to make predictions on time series data. Time series are a set of data points indexed in time order; such data points are obtained through a series of observations on the same sets of variables in successive points in time (points which are typically but not always equally spaced). Time series are thus a sequence of discrete-time data.\n",
    "If a time series contains observations regarding only one variable then it is called a **univariate time series**; in case a time series contains observations regarding a set of two or more variable then it is called a **multivariate time series**.\n",
    "\n",
    "In this practice lesson we will use a LSTM to predict the daily cases of COVID-19 in the world through an univariate time series. To do so we'll use the dataset \"csse_covid_19_time_series\", which contains the following features:\n",
    "  - \"Province/State\" the province or state from which the row of data is from\n",
    "  - \"Country/Region\", the country or region of the province or state the data refers to;\n",
    "  - \"Lat\", the latitude of the country/region;\n",
    "  - \"Long\", the longitude of the country/region;\n",
    "  - a series of dates with the number of cases in that day in a province/state. \n",
    "\n",
    "Let's first open the dataset and take a look at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NVxXGdqomwJO",
    "outputId": "1e664401-f2a0-4684-9890-813d8b8d4a3c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Province/State      Country/Region        Lat        Long  1/22/20  \\\n",
      "0              NaN         Afghanistan  33.939110   67.709953        0   \n",
      "1              NaN             Albania  41.153300   20.168300        0   \n",
      "2              NaN             Algeria  28.033900    1.659600        0   \n",
      "3              NaN             Andorra  42.506300    1.521800        0   \n",
      "4              NaN              Angola -11.202700   17.873900        0   \n",
      "..             ...                 ...        ...         ...      ...   \n",
      "268            NaN             Vietnam  14.058324  108.277199        0   \n",
      "269            NaN  West Bank and Gaza  31.952200   35.233200        0   \n",
      "270            NaN               Yemen  15.552727   48.516388        0   \n",
      "271            NaN              Zambia -13.133897   27.849332        0   \n",
      "272            NaN            Zimbabwe -19.015438   29.154857        0   \n",
      "\n",
      "     1/23/20  1/24/20  1/25/20  1/26/20  1/27/20  ...  1/14/21  1/15/21  \\\n",
      "0          0        0        0        0        0  ...    53775    53831   \n",
      "1          0        0        0        0        0  ...    65994    66635   \n",
      "2          0        0        0        0        0  ...   103127   103381   \n",
      "3          0        0        0        0        0  ...     8868     8946   \n",
      "4          0        0        0        0        0  ...    18613    18679   \n",
      "..       ...      ...      ...      ...      ...  ...      ...      ...   \n",
      "268        2        2        2        2        2  ...     1531     1536   \n",
      "269        0        0        0        0        0  ...   150505   151142   \n",
      "270        0        0        0        0        0  ...     2110     2111   \n",
      "271        0        0        0        0        0  ...    32800    34278   \n",
      "272        0        0        0        0        0  ...    25368    26109   \n",
      "\n",
      "     1/16/21  1/17/21  1/18/21  1/19/21  1/20/21  1/21/21  1/22/21  1/23/21  \n",
      "0      53938    53984    54062    54141    54278    54403    54483    54559  \n",
      "1      67216    67690    67982    68568    69238    69916    70655    71441  \n",
      "2     103611   103833   104092   104341   104606   104852   105124   105369  \n",
      "3       9038     9083     9083     9194     9308     9379     9416     9499  \n",
      "4      18765    18875    18926    19011    19093    19177    19269    19367  \n",
      "..       ...      ...      ...      ...      ...      ...      ...      ...  \n",
      "268     1537     1537     1539     1540     1544     1546     1548     1548  \n",
      "269   151569   152031   152555   153093   153590   154063   154557   155006  \n",
      "270     2112     2112     2113     2115     2115     2115     2118     2118  \n",
      "271    36074    37605    38207    39515    40949    42213    43333    44592  \n",
      "272    26881    27203    27892    28675    29408    30047    30523    31007  \n",
      "\n",
      "[273 rows x 372 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "ds = pd.read_csv(\"https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv\")\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i8J0d349qKzX"
   },
   "source": [
    "As it can seen by printing the dataset the data needs to be cleaned, hence the following steps are needed:\n",
    "  1. drop the first 4 columns because unneeded;\n",
    "  2. sum all rows to get a new dataset composed by the cumulative daily cases, this in order to pass from a multivariate time series to an univariate one;\n",
    "  3. check for missing values in the daily cases dataset and eliminate eventual rows with them;\n",
    "  4. split the daily cases dataset by using the first 200 days as training set and the remaining 68 as testing set;\n",
    "  5. scale the data with the MinMax scaler.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qlqE-TK5qONa",
    "outputId": "8b3099b0-d7c2-4df7-bec7-f95dc775447e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of 2020-01-22         557\n",
      "2020-01-23         655\n",
      "2020-01-24         941\n",
      "2020-01-25        1433\n",
      "2020-01-26        2118\n",
      "                ...   \n",
      "2021-01-19    96167933\n",
      "2021-01-20    96862056\n",
      "2021-01-21    97518881\n",
      "2021-01-22    98177108\n",
      "2021-01-23    98746982\n",
      "Length: 368, dtype: int64>\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "### Step 1. ###\n",
    "ds = ds.iloc[:, 4:] ### CODE HERE ###\n",
    "\n",
    "### Step 2. ###\n",
    "daily_cases = ds.sum(axis=0) ### CODE HERE ###\n",
    "daily_cases.index = pd.to_datetime(daily_cases.index) ### CODE HERE ###\n",
    "print(daily_cases.head)\n",
    "\n",
    "### Step 3. ###\n",
    "number_of_nans = daily_cases.isnull().sum().sum() ### CODE HERE ###\n",
    "print(number_of_nans)\n",
    "\n",
    "### Step 4. ###\n",
    "train_data = daily_cases[:-68] ### CODE HERE ###\n",
    "test_data = daily_cases[-68:] ### CODE HERE ###\n",
    "\n",
    "### Step 5. ###\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler = scaler.fit(np.expand_dims(train_data, axis=1))  # expand_dims is needed to insert a new axis during the scaling operation to assure that the MinMaxScaler performs correctly\n",
    "\n",
    "train_data = scaler.transform(np.expand_dims(train_data, axis=1)) ### CODE HERE ###\n",
    "test_data = scaler.transform(np.expand_dims(test_data, axis=1)) ### CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rwjmC1ZSq_Jn"
   },
   "source": [
    "Currently we have a big sequence of daily cases so we’ll convert it into smaller ones (of length seq_length=5) in order to ease the training process of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Drr5FyoWq_lr"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def create_sequences(data, seq_length=5):\n",
    "    \n",
    "    xs = []  # data sequence\n",
    "    ys = []  # target sequence\n",
    "\n",
    "    for i in range(len(data)-seq_length-1):\n",
    "        x = data[i:(i+seq_length)]\n",
    "        y = data[i+seq_length]\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "    \n",
    "    return np.array(xs), np.array(ys)\n",
    "\n",
    "X_train, y_train = create_sequences(train_data) ### CODE HERE ###  # creating the training data sequences\n",
    "X_test, y_test = create_sequences(test_data) ### CODE HERE ###  # creating the test data sequences\n",
    "\n",
    "# casting the sequences to float numbers\n",
    "X_train = torch.from_numpy(X_train).float() ### CODE HERE ###\n",
    "y_train = torch.from_numpy(y_train).float() ### CODE HERE ###\n",
    "\n",
    "X_test = torch.from_numpy(X_test).float()### CODE HERE ###\n",
    "y_test = torch.from_numpy(y_test).float()### CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sj61E0TYrKDu"
   },
   "source": [
    "Now it's time to incapsulate the model in a torch class. We'll make a model in which a linear layer takes in input the data calculated by a LSTM network. The class will have as parameters the number of features in input, the number of hidden layers, the length of the sequence and the number of LSTM layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "hJVqpQUErLkT"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class TimeSeriesPredictor(nn.Module):\n",
    "\n",
    "  def __init__(self, n_features=1, n_hidden=512, seq_len=5, n_layers=1, out_features=1):\n",
    "    super(TimeSeriesPredictor, self).__init__()\n",
    "\n",
    "    self.n_hidden = n_hidden ### CODE HERE ###\n",
    "    self.seq_len = seq_len ### CODE HERE ###\n",
    "    self.n_layers = n_layers### CODE HERE ###\n",
    "    self.out_features = out_features ### CODE HERE ###\n",
    "\n",
    "    self.lstm = nn.LSTM(input_size=n_features, hidden_size=n_hidden, num_layers=n_layers) ### CODE HERE ###\n",
    "    \n",
    "    self.linear = nn.Linear(in_features=n_hidden, out_features=self.out_features) ### CODE HERE ###\n",
    "\n",
    "  def forward(self, sequences):\n",
    "    # output of the LSTM and updated hidden state of the LSTM\n",
    "    lstm_out, self.hidden = self.lstm(sequences.view(len(sequences),\n",
    "                                                     self.seq_len, -1),\n",
    "                                      self.hidden)\n",
    "    \n",
    "    # last time step handled by the LSTM\n",
    "    last_time_step = lstm_out.view(self.seq_len,\n",
    "                                   len(sequences), self.n_hidden)[-1]\n",
    "\n",
    "    # prediction on the value of the next time step (number of new cases the following day)                                \n",
    "    y_pred = self.linear(last_time_step) ### CODE HERE ###\n",
    "\n",
    "    return y_pred\n",
    "  \n",
    "  # Function needed to reset the hidden state  \n",
    "  def reset_hidden_state(self):\n",
    "    self.hidden = (\n",
    "        torch.zeros(self.n_layers, self.seq_len, self.n_hidden),\n",
    "        torch.zeros(self.n_layers, self.seq_len, self.n_hidden)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q2xOinLyrjqC"
   },
   "source": [
    "We now have to define the function that handles the training of the LSTM-based model. As optimizer we'll use Adam with learning rate 1e-3, while as cost function we'll use MSELoss; we'll train the model for 100 epochs. We'll print the train and test loss values every 10 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "v1htX_nUru2w"
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_data, train_labels, test_data=None, \n",
    "                test_labels=None):\n",
    "  \n",
    "  loss_fn = torch.nn.MSELoss(reduction='sum')  # initializing the loss function\n",
    "  \n",
    "  optimiser = torch.optim.Adam(model.parameters(), lr=1e-3)  # initializing the optimizer\n",
    "  \n",
    "  num_epochs = 100\n",
    "  \n",
    "  train_loss_hist = np.zeros(num_epochs)  # training loss history\n",
    "  test_loss_hist = np.zeros(num_epochs)  # testing loss history\n",
    "  \n",
    "  for t in range(num_epochs):\n",
    "\n",
    "    model.reset_hidden_state() # resetting the hidden state\n",
    "    \n",
    "    y_pred = model(X_train)  # output on the sequence\n",
    "\n",
    "    loss = loss_fn(y_pred.float(), y_train)  # loss calculation\n",
    "    \n",
    "    if test_data is not None:\n",
    "\n",
    "      with torch.no_grad():\n",
    "\n",
    "        y_test_pred = model(X_test)\n",
    "        test_loss = loss_fn(y_test_pred.float(), y_test)\n",
    "      \n",
    "      test_loss_hist[t] = test_loss.item()\n",
    "\n",
    "      if t % 10 == 0 or t==99:\n",
    "        print(f\"Epoch {t} train loss: {loss.item()}, \"\n",
    "          f\"test loss: {test_loss.item()}\")\n",
    "    \n",
    "    elif t % 10 == 0 or t==99:\n",
    "      print(f'Epoch {t} train loss: {loss.item()}')\n",
    "    \n",
    "    train_loss_hist[t] = loss.item()\n",
    "\n",
    "    optimiser.zero_grad()  # zeroing the gradients\n",
    "    loss.backward()  # backward pass\n",
    "    optimiser.step()  # optimization step\n",
    "\n",
    "  return model.eval(), train_loss_hist, test_loss_hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZT0pvun7rytV"
   },
   "source": [
    "Let's now run our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-4XUOIQVr2th",
    "outputId": "86a9d3d0-50d0-4739-df78-593bdb4a5f44"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train loss: 54.07327651977539, test loss: 137.7874755859375\n",
      "Epoch 10 train loss: 21.47332191467285, test loss: 72.49911499023438\n",
      "Epoch 20 train loss: 17.60870361328125, test loss: 39.137428283691406\n",
      "Epoch 30 train loss: 58.51490020751953, test loss: 1165.729736328125\n",
      "Epoch 40 train loss: 58.866493225097656, test loss: 117.69242095947266\n",
      "Epoch 50 train loss: 28.999780654907227, test loss: 92.35807800292969\n",
      "Epoch 60 train loss: 21.2719669342041, test loss: 65.94776916503906\n",
      "Epoch 70 train loss: 21.163148880004883, test loss: 64.65777587890625\n",
      "Epoch 80 train loss: 21.05211067199707, test loss: 69.88024139404297\n",
      "Epoch 90 train loss: 20.76102638244629, test loss: 66.59771728515625\n",
      "Epoch 99 train loss: 20.585508346557617, test loss: 62.56349563598633\n"
     ]
    }
   ],
   "source": [
    "model = TimeSeriesPredictor() ### CODE HERE ###\n",
    "\n",
    "model, train_loss_hist, test_loss_hist = train_model(model, X_train, y_train, X_test, y_test) ### CODE HERE ###"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "09_Time_series_forecasting_with_LSTMs.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
