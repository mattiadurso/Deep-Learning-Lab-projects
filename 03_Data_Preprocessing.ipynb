{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8DONiNhwvbij"
   },
   "source": [
    "# Data Preprocessing\n",
    "The preprocessing of the features of a dataset is a fundamental step for the preparation of data to be fed to a machine learning algorithm. In fact, while the steps of dataset design and of data analysis are made to ensure that a dataset contains only meaningful data, the step of data preprocessing ensures that the data representation is fit to be given in input to a neural network.\n",
    "\n",
    "The preprocessing of a dataset consists of 4 phases:\n",
    "1.   identifying and handling missing values;\n",
    "2.   encoding the categorical features;\n",
    "3.   splitting the dataset in training and test set;\n",
    "4.   scaling the features of the dataset.\n",
    "\n",
    "In the next sections of this practical lesson we'll see how to handle the preprocessing of the Titanic Survivals dataset, so let's import it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "id": "ZEvGAngL5N7N"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings; Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen; Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle; Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen; Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Moran; Mr. James</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>330877</td>\n",
       "      <td>8.4583</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>McCarthy; Mr. Timothy J</td>\n",
       "      <td>male</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17463</td>\n",
       "      <td>51.8625</td>\n",
       "      <td>E46</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Palsson; Master. Gosta Leonard</td>\n",
       "      <td>male</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>349909</td>\n",
       "      <td>21.0750</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Johnson; Mrs. Oscar W (Elisabeth Vilhelmina Berg)</td>\n",
       "      <td>female</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>347742</td>\n",
       "      <td>11.1333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Nasser; Mrs. Nicholas (Adele Achem)</td>\n",
       "      <td>female</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>237736</td>\n",
       "      <td>30.0708</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "5            6         0       3   \n",
       "6            7         0       1   \n",
       "7            8         0       3   \n",
       "8            9         1       3   \n",
       "9           10         1       2   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "1  Cumings; Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen; Miss. Laina  female  26.0      0   \n",
       "3       Futrelle; Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen; Mr. William Henry    male  35.0      0   \n",
       "5                                   Moran; Mr. James    male   NaN      0   \n",
       "6                            McCarthy; Mr. Timothy J    male  54.0      0   \n",
       "7                     Palsson; Master. Gosta Leonard    male   2.0      3   \n",
       "8  Johnson; Mrs. Oscar W (Elisabeth Vilhelmina Berg)  female  27.0      0   \n",
       "9                Nasser; Mrs. Nicholas (Adele Achem)  female  14.0      1   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  \n",
       "5      0            330877   8.4583   NaN        Q  \n",
       "6      0             17463  51.8625   E46        S  \n",
       "7      1            349909  21.0750   NaN        S  \n",
       "8      2            347742  11.1333   NaN        S  \n",
       "9      0            237736  30.0708   NaN        C  "
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "df = pd.read_csv('http://ailab.uniud.it/wp-content/uploads/2020/10/titanic_survivals.csv', sep=',', index_col=None)  # Loading the dataset in a Pandas Dataframe\n",
    "df[1:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VYjqOD7051-l"
   },
   "source": [
    "# Handling missing values\n",
    "As we have seen when printing the first ten values of the dataset, there are some columns that have missing values, marked with the value *NaN* (Not a Number). Such undefined values, while usually ignored by Pandas operations like *median()* and *mean()*, can cause problems in both other built-in and new user-defined operations; moreover, such missing values cannot be handled at all by a neural network. In order to avoid future problems, we have to get rid of the missing values of the dataset. This can be done in three ways:\n",
    "\n",
    "1.   by picking a strategy to fill the holes in the dataset;\n",
    "2.   by dropping from the dataset the rows that have a missing value;\n",
    "3.   by dropping an entire column from the dataset if it has too many missing values that cannot be replaced.\n",
    "\n",
    "Such strategies are not mutually exclusive and can be used together to handle the missing values in the dataset. In order to handle the missing values in the dataset, let's check which columns have missing values and how many missing values they have. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "id": "pVLSieCa57RK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The column PassengerId has 0 missing values\n",
      "\n",
      "The column Survived has 0 missing values\n",
      "\n",
      "The column Pclass has 0 missing values\n",
      "\n",
      "The column Name has 0 missing values\n",
      "\n",
      "The column Sex has 0 missing values\n",
      "\n",
      "The column Age has 177 missing values\n",
      "\n",
      "The column SibSp has 0 missing values\n",
      "\n",
      "The column Parch has 0 missing values\n",
      "\n",
      "The column Ticket has 0 missing values\n",
      "\n",
      "The column Fare has 0 missing values\n",
      "\n",
      "The column Cabin has 687 missing values\n",
      "\n",
      "The column Embarked has 2 missing values\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for column in df.columns:\n",
    "  \n",
    "  number_of_nans_in_column = df[column].isnull().sum()  # Counting the number of missing values in the column\n",
    "  print(f\"The column {column} has {number_of_nans_in_column} missing values\\n\")  # Printing the column with missing values and how many such values are in it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-MNUmG1dLGsP"
   },
   "source": [
    "As we can see, the column with the most missing entries is Cabin, with 687 missing values, while the column Age has 177 missing values. The column with the least missing entries is Embarked, with only 2 missing values.\n",
    "\n",
    "Since the column Cabin has too many missing values (687 out of 891) to try using the other two approaches, the best course of action is to drop the column altogether from the dataset. In order to do so, we have to use the Pandas function *drop()* while specifying the column we want to drop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "id": "YE1gjngqWHJO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(891, 11)\n"
     ]
    }
   ],
   "source": [
    "df = df.drop(columns = ['Cabin'])# Dropping the Cabin column\n",
    "\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "20FQ8fpQ5-Yw"
   },
   "source": [
    "As we can see by printing the new shape of the dataframe, the dataset now has one less column.\n",
    "\n",
    "Now, in order to handle the missing values in the column Age, there are two possible courses of action: either eliminating from the dataset the rows which have a *NaN* value in the Age column or replacing the missing values with a new value. In some cases dropping the NaNs from a dataset isn't a viable solution because of the excessive number of lines dropped. Some of the most common strategies used to replace NaN values without eliminating the row from the dataset (commonly called **data filling** strategies) are:\n",
    "*    replacing the NaNs with the most common value found in the dataset column (*median filling*);\n",
    "*    replacing the NaNs with the mean value of the dataset column (*mean filling*);\n",
    "*    propagating the last valid observation forward to the next valid one (*forward filling*)\n",
    "*    propagating the last valid observation backward to the previous valid one (backward filling).\n",
    "\n",
    "By eliminating from the dataset the rows which have a *NaN* value in the Age column we would erase 177 rows from the dataset, which are roughly the 20% of the entire dataset. Since those are too many values to drop, the best alternative is to replace the *NaN* values with a new value.\n",
    "\n",
    "Let's try filling the NaN values of the column \"Age\" of the dataset by replacing the *NaN* values with a mean filling of the age of the passengers. We'll do so by using the *replace()* command of Pandas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "id": "T51Ihdx0cEoH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.69911764705882\n",
      "29.699118    177\n",
      "24.000000     30\n",
      "22.000000     27\n",
      "18.000000     26\n",
      "28.000000     25\n",
      "            ... \n",
      "55.500000      1\n",
      "53.000000      1\n",
      "20.500000      1\n",
      "23.500000      1\n",
      "0.420000       1\n",
      "Name: Age, Length: 89, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "mean_age_passengers = df[\"Age\"].mean()  # Getting the mean age of all the passengers\n",
    "print(mean_age_passengers)\n",
    "\n",
    "# IMPORTANT! In dataframes the NaN values correspond to the value returned by the \"np.nan\" command of the library numpy (here shorted in np)\n",
    "column_without_nans = df[\"Age\"].replace(np.nan,mean_age_passengers ) # Replacing in the Age column the NaN values with the mean age of the passengers\n",
    "\n",
    "df[\"Age\"] = column_without_nans # Assigning to the original column\n",
    "\n",
    "print(df[\"Age\"].value_counts())  # By using value_counts() we can see we replaced successfully the 177 NaN values in the Age column with the mean age of the passengers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ATjrik6QAbAF"
   },
   "source": [
    "Now that we've dealt with the missing values of the column Age, the last column with *NaN* entries is the Embarked one. Since there are only 2 empty entries in such column, the easiest way to proceed is to drop the rows with such entries from the dataset with the *dropna()* command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "id": "8l-_B_yu6DFx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(889, 12)\n"
     ]
    }
   ],
   "source": [
    "df = df.dropna()# Applying dropna to the dataset\n",
    "df.reset_index(inplace=True)  # IMPORTANT! When removing rows from a dataset it's always recommended to reset the index of the dataframe \n",
    "print(df.shape)  # Printing the new shape of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-B8j9l6T6C16"
   },
   "source": [
    "Through the command dropna the number of rows of the dataset passed from 891 to 889, and so only the 2 rows containing *NaN* values were dropped. Now let's check again that the dataset doesn't have empty entries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "id": "kF18J5HO6k8_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The column index has 0 missing values\n",
      "\n",
      "The column PassengerId has 0 missing values\n",
      "\n",
      "The column Survived has 0 missing values\n",
      "\n",
      "The column Pclass has 0 missing values\n",
      "\n",
      "The column Name has 0 missing values\n",
      "\n",
      "The column Sex has 0 missing values\n",
      "\n",
      "The column Age has 0 missing values\n",
      "\n",
      "The column SibSp has 0 missing values\n",
      "\n",
      "The column Parch has 0 missing values\n",
      "\n",
      "The column Ticket has 0 missing values\n",
      "\n",
      "The column Fare has 0 missing values\n",
      "\n",
      "The column Embarked has 0 missing values\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset_has_nans = False\n",
    "\n",
    "for column in df.columns:\n",
    "  \n",
    "  number_of_nans_in_column =  df[column].isnull().sum()# Counting the number of missing values in the column\n",
    "  print(f\"The column {column} has {number_of_nans_in_column} missing values\\n\")  # Printing the column with missing values and how many such values are in it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xhmu8pPlDY59"
   },
   "source": [
    "# Encoding categorical features\n",
    "The categorical features of a dataset are all those features that don't actually represent values, but categories; such categories can be represented as strings or characters identifying a certain class. Neural networks don't know how to handle strings or characters and so that type of data needs to be encoded in a numerical form, so that the neural network can process it.\n",
    "\n",
    "Instances of categorical features in the Titanic Survivals dataset are the column Pclass, in which the category of the ticket class of the passenger is *already encoded* through an integer (1 = 1st, 2 = 2nd, 3 = 3rd class), the column Sex, in which the category of the sex of the passenger is represented through a string (\"Male\" for men, \"Female\" for women) that needs to be encoded, and the column Embarked, in which the category of the port of embarkation is represented through a character that needs to be encoded ('C'=Cherbourg, 'Q'=Queenstown, 'S'=Southampton).\n",
    "\n",
    "Among the most popular methods used to encode categorical features the two most used approaches are **integer encoding** and a **one-hot encoding**.\n",
    "\n",
    "With the integer encoding (which was used for the column Pclass) to each unique category value of a feature is assigned an integer, which replaces the original value. While simple, integer encoding should be used *carefully*, since with such encoding it is possible to enstablish an ordinal relationship where there is not supposed to be one. For instance, let's say we map the values of the feature Sex in the following way:\n",
    "\n",
    "'Male' -> 0; 'Female' -> 1;\n",
    "\n",
    "by doing so we also implicitly define an ordinal relationship, since 1>0, but such relationship doesn't apply to the original data (the value \"Female\" isn't \"greater than\" the value \"Male\").\n",
    "\n",
    "The one-hot encoding is often used to avoid the issues of the integer encoding. In one-hot encoding the feature to be encoded is removed and a new binary variable is added for each unique categorical value.  In the Sex variable example, there are 2 categories (\"Male\" and \"Female\") and therefore 2 binary variables are needed. A \"1\" value is placed in the binary variable of the variable to be encoded and \"0\" values for the other values. Let's see an example by using only the first 10 rows of the original dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "id": "4a4LFi2_Ob5H"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   index  PassengerId  Survived  Pclass  \\\n",
      "0      0            1         0       3   \n",
      "1      1            2         1       1   \n",
      "2      2            3         1       3   \n",
      "3      3            4         1       1   \n",
      "4      4            5         0       3   \n",
      "5      5            6         0       3   \n",
      "6      6            7         0       1   \n",
      "7      7            8         0       3   \n",
      "8      8            9         1       3   \n",
      "9      9           10         1       2   \n",
      "\n",
      "                                                Name     Sex        Age  \\\n",
      "0                            Braund; Mr. Owen Harris    male  22.000000   \n",
      "1  Cumings; Mrs. John Bradley (Florence Briggs Th...  female  38.000000   \n",
      "2                             Heikkinen; Miss. Laina  female  26.000000   \n",
      "3       Futrelle; Mrs. Jacques Heath (Lily May Peel)  female  35.000000   \n",
      "4                           Allen; Mr. William Henry    male  35.000000   \n",
      "5                                   Moran; Mr. James    male  29.699118   \n",
      "6                            McCarthy; Mr. Timothy J    male  54.000000   \n",
      "7                     Palsson; Master. Gosta Leonard    male   2.000000   \n",
      "8  Johnson; Mrs. Oscar W (Elisabeth Vilhelmina Berg)  female  27.000000   \n",
      "9                Nasser; Mrs. Nicholas (Adele Achem)  female  14.000000   \n",
      "\n",
      "   SibSp  Parch            Ticket     Fare Embarked  \n",
      "0      1      0         A/5 21171   7.2500        S  \n",
      "1      1      0          PC 17599  71.2833        C  \n",
      "2      0      0  STON/O2. 3101282   7.9250        S  \n",
      "3      1      0            113803  53.1000        S  \n",
      "4      0      0            373450   8.0500        S  \n",
      "5      0      0            330877   8.4583        Q  \n",
      "6      0      0             17463  51.8625        S  \n",
      "7      3      1            349909  21.0750        S  \n",
      "8      0      2            347742  11.1333        S  \n",
      "9      1      0            237736  30.0708        C  \n",
      "Unencoded values of the column Sex:\n",
      "      Sex\n",
      "0    male\n",
      "1  female\n",
      "2  female\n",
      "3  female\n",
      "4    male\n",
      "5    male\n",
      "6    male\n",
      "7    male\n",
      "8  female\n",
      "9  female\n",
      "\n",
      "Array of the values encoded by the one-hot encoder:\n",
      "[[0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]]\n",
      "\n",
      "Dataframe of the values encoded by the one-hot encoder:\n",
      "   isFemale  isMale\n",
      "0       0.0     1.0\n",
      "1       1.0     0.0\n",
      "2       1.0     0.0\n",
      "3       1.0     0.0\n",
      "4       0.0     1.0\n",
      "5       0.0     1.0\n",
      "6       0.0     1.0\n",
      "7       0.0     1.0\n",
      "8       1.0     0.0\n",
      "9       1.0     0.0\n",
      "\n",
      "Dataframe with the encoded values:\n",
      "   index  PassengerId  Survived  Pclass  \\\n",
      "0      0            1         0       3   \n",
      "1      1            2         1       1   \n",
      "2      2            3         1       3   \n",
      "3      3            4         1       1   \n",
      "4      4            5         0       3   \n",
      "5      5            6         0       3   \n",
      "6      6            7         0       1   \n",
      "7      7            8         0       3   \n",
      "8      8            9         1       3   \n",
      "9      9           10         1       2   \n",
      "\n",
      "                                                Name        Age  SibSp  Parch  \\\n",
      "0                            Braund; Mr. Owen Harris  22.000000      1      0   \n",
      "1  Cumings; Mrs. John Bradley (Florence Briggs Th...  38.000000      1      0   \n",
      "2                             Heikkinen; Miss. Laina  26.000000      0      0   \n",
      "3       Futrelle; Mrs. Jacques Heath (Lily May Peel)  35.000000      1      0   \n",
      "4                           Allen; Mr. William Henry  35.000000      0      0   \n",
      "5                                   Moran; Mr. James  29.699118      0      0   \n",
      "6                            McCarthy; Mr. Timothy J  54.000000      0      0   \n",
      "7                     Palsson; Master. Gosta Leonard   2.000000      3      1   \n",
      "8  Johnson; Mrs. Oscar W (Elisabeth Vilhelmina Berg)  27.000000      0      2   \n",
      "9                Nasser; Mrs. Nicholas (Adele Achem)  14.000000      1      0   \n",
      "\n",
      "             Ticket     Fare Embarked  isFemale  isMale  \n",
      "0         A/5 21171   7.2500        S       0.0     1.0  \n",
      "1          PC 17599  71.2833        C       1.0     0.0  \n",
      "2  STON/O2. 3101282   7.9250        S       1.0     0.0  \n",
      "3            113803  53.1000        S       1.0     0.0  \n",
      "4            373450   8.0500        S       0.0     1.0  \n",
      "5            330877   8.4583        Q       0.0     1.0  \n",
      "6             17463  51.8625        S       0.0     1.0  \n",
      "7            349909  21.0750        S       0.0     1.0  \n",
      "8            347742  11.1333        S       1.0     0.0  \n",
      "9            237736  30.0708        C       1.0     0.0  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataframe_to_encode = df.head(10)  # Getting the first 10 rows of the original dataset\n",
    "print(dataframe_to_encode)\n",
    "\n",
    "values_to_encode = dataframe_to_encode[[\"Sex\"]]  # Inserting the values to be encoded in a dataframe (hence the double square brackets)\n",
    "print(f\"Unencoded values of the column Sex:\\n{values_to_encode}\\n\")\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder  # The most used implementation of the one-hot encoder is the one of the sklearn library for preprocessing \n",
    "encoder = OneHotEncoder()  # Initializing the encoder\n",
    "encoded_values = encoder.fit_transform(values_to_encode)  # Fitting the encoder with the data to encode and making it transform such data\n",
    "encoded_values = encoded_values.toarray()  # Putting the encoded values in a numpy array\n",
    "print(f\"Array of the values encoded by the one-hot encoder:\\n{encoded_values}\\n\")  # Checking which categorical value was encoded first\n",
    "\n",
    "# Converting the encoded values in a dataframe, while naming the new columns according to the order in which the values were encoded\n",
    "encoded_values = pd.DataFrame(encoded_values, columns=[\"isFemale\", \"isMale\"])\n",
    "print(f\"Dataframe of the values encoded by the one-hot encoder:\\n{encoded_values}\\n\")\n",
    "\n",
    "dataframe_to_encode = dataframe_to_encode.join(encoded_values)  # Adding the columns with the encoded values to the dataframe\n",
    "dataframe_to_encode = dataframe_to_encode.drop(columns=[\"Sex\"])  # Dropping the original unencoded column\n",
    "print(f\"Dataframe with the encoded values:\\n{dataframe_to_encode}\\n\")  # Printing the dataset with the encoded data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "teEm8LA7ZQq-"
   },
   "source": [
    "With one-hot encoding the original column of the dataset is **replaced** by *n* new columns, where *n* is the number of unique categorical values of the feature to be encoded. One-hot encoding can cause problems when a categorical feature has too many unique categorical values. For instance, encoding a categorical feature of a dataset with 300 unique categorical values with one-hot encoding adds 300 new columns to the dataset, while only the original column is dropped, thus effectively adding 299 new columns to the dataset!\n",
    "\n",
    "Now let's try use one-hot encoding to encode the feature Embarked, since such feature has only 3 unique categorical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "id": "z9JJ_oGxmTML"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First ten values of the feature to be encoded:\n",
      "  Embarked\n",
      "0        S\n",
      "1        C\n",
      "2        S\n",
      "3        S\n",
      "4        S\n",
      "5        Q\n",
      "6        S\n",
      "7        S\n",
      "8        S\n",
      "9        C\n",
      "\n",
      "First ten values of the encoded feature:\n",
      "[[0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]]\n",
      "\n",
      "Dataframe with the encoded values:\n",
      "     index  PassengerId  Survived  Pclass  \\\n",
      "0        0            1         0       3   \n",
      "1        1            2         1       1   \n",
      "2        2            3         1       3   \n",
      "3        3            4         1       1   \n",
      "4        4            5         0       3   \n",
      "..     ...          ...       ...     ...   \n",
      "884    886          887         0       2   \n",
      "885    887          888         1       1   \n",
      "886    888          889         0       3   \n",
      "887    889          890         1       1   \n",
      "888    890          891         0       3   \n",
      "\n",
      "                                                  Name     Sex        Age  \\\n",
      "0                              Braund; Mr. Owen Harris    male  22.000000   \n",
      "1    Cumings; Mrs. John Bradley (Florence Briggs Th...  female  38.000000   \n",
      "2                               Heikkinen; Miss. Laina  female  26.000000   \n",
      "3         Futrelle; Mrs. Jacques Heath (Lily May Peel)  female  35.000000   \n",
      "4                             Allen; Mr. William Henry    male  35.000000   \n",
      "..                                                 ...     ...        ...   \n",
      "884                              Montvila; Rev. Juozas    male  27.000000   \n",
      "885                       Graham; Miss. Margaret Edith  female  19.000000   \n",
      "886             Johnston; Miss. Catherine Helen Carrie  female  29.699118   \n",
      "887                              Behr; Mr. Karl Howell    male  26.000000   \n",
      "888                                Dooley; Mr. Patrick    male  32.000000   \n",
      "\n",
      "     SibSp  Parch            Ticket     Fare  inCherbourg  inQueenstown  \\\n",
      "0        1      0         A/5 21171   7.2500          0.0           0.0   \n",
      "1        1      0          PC 17599  71.2833          1.0           0.0   \n",
      "2        0      0  STON/O2. 3101282   7.9250          0.0           0.0   \n",
      "3        1      0            113803  53.1000          0.0           0.0   \n",
      "4        0      0            373450   8.0500          0.0           0.0   \n",
      "..     ...    ...               ...      ...          ...           ...   \n",
      "884      0      0            211536  13.0000          0.0           0.0   \n",
      "885      0      0            112053  30.0000          0.0           0.0   \n",
      "886      1      2        W./C. 6607  23.4500          0.0           0.0   \n",
      "887      0      0            111369  30.0000          1.0           0.0   \n",
      "888      0      0            370376   7.7500          0.0           1.0   \n",
      "\n",
      "     inSouthampton  \n",
      "0              1.0  \n",
      "1              0.0  \n",
      "2              1.0  \n",
      "3              1.0  \n",
      "4              1.0  \n",
      "..             ...  \n",
      "884            1.0  \n",
      "885            1.0  \n",
      "886            1.0  \n",
      "887            0.0  \n",
      "888            0.0  \n",
      "\n",
      "[889 rows x 14 columns]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = d2\n",
    "\n",
    "values_to_encode = df[[\"Embarked\"]]  # Selecting the values to be encoded in a dataframe\n",
    "print(f\"First ten values of the feature to be encoded:\\n{values_to_encode.head(10)}\\n\")  # Checking the values of the unencoded categorical values\n",
    "\n",
    "encoder = OneHotEncoder()\n",
    "encoded_values = encoder.fit_transform(values_to_encode)\n",
    "encoded_values = encoded_values.toarray()\n",
    "print(f\"First ten values of the encoded feature:\\n{encoded_values[:10]}\\n\")  # Checking the order in which the categorical values were encoded\n",
    "\n",
    "encoded_values = pd.DataFrame(encoded_values, columns=[\"inCherbourg\", \"inQueenstown\", \"inSouthampton\"])\n",
    "\n",
    "df = df.join(encoded_values)\n",
    "df = df.drop(columns=['Embarked'])\n",
    "print(f\"Dataframe with the encoded values:\\n{df}\\n\")  # Printing the dataset with the encoded data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lVxqxD-LJz16"
   },
   "source": [
    "# Splitting the dataset\n",
    "Splitting the dataset before giving the data in input to a neural network is a fundamental step in data preprocessing. Datasets are commonly splitted in two disjointed parts:\n",
    "*   a **training set**, which data is used during the training phase of a machine learning algorithm in order to fit the parameters (weights) of the neural network to the data given in input;\n",
    "*   a **testing set**, used to benchmark the performances of the trained neural network.\n",
    "\n",
    "As a rule of thumb, datasets are typically divided among training and testing set with a 80-20 proportion.\n",
    "\n",
    "Let's see how to split the Titanic Survival dataset by using the sklearn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "id": "AdLW8tepJ1yj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training set is:\n",
      "(711, 14)\n",
      "\n",
      "The testing set is:\n",
      "(178, 14)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split  # Importing the splitting function from the model_selection functionalities of sklearn\n",
    "\n",
    "train, test = train_test_split(df, test_size=0.2)  # Appyling the splitting function to the dataset, which randomly selects 20% of the dataset rows for the testing set\n",
    "\n",
    "print(f\"The training set is:\\n{train.shape}\\n\")\n",
    "print(f\"The testing set is:\\n{test.shape}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V6LkNHVOM0AU"
   },
   "source": [
    "Sometimes the dataset can also be further divided in order to obtain a **validation set**. A validation dataset is a dataset of examples used to tune the hyperparameters of a neural network in order to avoid the problem of overfitting during the training process, thus improving the performance of the network in the testing phase.\n",
    "\n",
    "When using a training, validation and testing set, the original dataset is tipically splitted with a 60-20-20 proportion.\n",
    "\n",
    "Now let's try dividing the training set in order to obtain a validation dataset following such proportion (and thus with the same number of rows of the testing set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "id": "dHbm9-8RMZkw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The validation and testing sets have the same number of rows!\n",
      "The new training set is:\n",
      "     index  PassengerId  Survived  Pclass  \\\n",
      "504    505          506         0       1   \n",
      "252    253          254         0       3   \n",
      "665    666          667         0       2   \n",
      "514    515          516         0       1   \n",
      "487    488          489         0       3   \n",
      "..     ...          ...       ...     ...   \n",
      "568    569          570         1       3   \n",
      "790    791          792         0       2   \n",
      "77      78           79         1       2   \n",
      "386    387          388         1       2   \n",
      "831    833          834         0       3   \n",
      "\n",
      "                                           Name     Sex    Age  SibSp  Parch  \\\n",
      "504  Penasco y Castellana; Mr. Victor de Satode    male  18.00      1      0   \n",
      "252                    Lobb; Mr. William Arthur    male  30.00      1      0   \n",
      "665                 Butler; Mr. Reginald Fenton    male  25.00      0      0   \n",
      "514                Walker; Mr. William Anderson    male  47.00      0      0   \n",
      "487               Somerton; Mr. Francis William    male  30.00      0      0   \n",
      "..                                          ...     ...    ...    ...    ...   \n",
      "568                           Jonsson; Mr. Carl    male  32.00      0      0   \n",
      "790                         Gaskell; Mr. Alfred    male  16.00      0      0   \n",
      "77                Caldwell; Master. Alden Gates    male   0.83      0      2   \n",
      "386                            Buss; Miss. Kate  female  36.00      0      0   \n",
      "831                      Augustsson; Mr. Albert    male  23.00      0      0   \n",
      "\n",
      "         Ticket      Fare  inCherbourg  inQueenstown  inSouthampton  \n",
      "504    PC 17758  108.9000          1.0           0.0            0.0  \n",
      "252   A/5. 3336   16.1000          0.0           0.0            1.0  \n",
      "665      234686   13.0000          0.0           0.0            1.0  \n",
      "514       36967   34.0208          0.0           0.0            1.0  \n",
      "487  A.5. 18509    8.0500          0.0           0.0            1.0  \n",
      "..          ...       ...          ...           ...            ...  \n",
      "568      350417    7.8542          0.0           0.0            1.0  \n",
      "790      239865   26.0000          0.0           0.0            1.0  \n",
      "77       248738   29.0000          0.0           0.0            1.0  \n",
      "386       27849   13.0000          0.0           0.0            1.0  \n",
      "831      347468    7.8542          0.0           0.0            1.0  \n",
      "\n",
      "[533 rows x 14 columns]\n",
      "\n",
      "The validation set is:\n",
      "     index  PassengerId  Survived  Pclass  \\\n",
      "885    887          888         1       1   \n",
      "863    865          866         1       2   \n",
      "100    101          102         0       3   \n",
      "380    381          382         1       3   \n",
      "225    226          227         1       2   \n",
      "..     ...          ...       ...     ...   \n",
      "293    294          295         0       3   \n",
      "449    450          451         0       2   \n",
      "771    772          773         0       2   \n",
      "299    300          301         1       3   \n",
      "387    388          389         0       3   \n",
      "\n",
      "                                       Name     Sex        Age  SibSp  Parch  \\\n",
      "885            Graham; Miss. Margaret Edith  female  19.000000      0      0   \n",
      "863                Bystrom; Mrs. (Karolina)  female  42.000000      0      0   \n",
      "100          Petroff; Mr. Pastcho (Pentcho)    male  29.699118      0      0   \n",
      "380               Nakid; Miss. Maria (Mary)  female   1.000000      0      2   \n",
      "225               Mellors; Mr. William John    male  19.000000      0      0   \n",
      "..                                      ...     ...        ...    ...    ...   \n",
      "293                        Mineff; Mr. Ivan    male  24.000000      0      0   \n",
      "449                   West; Mr. Edwy Arthur    male  36.000000      1      2   \n",
      "771                       Mack; Mrs. (Mary)  female  57.000000      0      0   \n",
      "299  Kelly; Miss. Anna Katherine Annie Kate  female  29.699118      0      0   \n",
      "387                    Sadlier; Mr. Matthew    male  29.699118      0      0   \n",
      "\n",
      "          Ticket     Fare  inCherbourg  inQueenstown  inSouthampton  \n",
      "885       112053  30.0000          0.0           0.0            1.0  \n",
      "863       236852  13.0000          0.0           0.0            1.0  \n",
      "100       349215   7.8958          0.0           0.0            1.0  \n",
      "380         2653  15.7417          1.0           0.0            0.0  \n",
      "225    SW/PP 751  10.5000          0.0           0.0            1.0  \n",
      "..           ...      ...          ...           ...            ...  \n",
      "293       349233   7.8958          0.0           0.0            1.0  \n",
      "449   C.A. 34651  27.7500          0.0           0.0            1.0  \n",
      "771  S.O./P.P. 3  10.5000          0.0           0.0            1.0  \n",
      "299         9234   7.7500          0.0           1.0            0.0  \n",
      "387       367655   7.7292          0.0           1.0            0.0  \n",
      "\n",
      "[178 rows x 14 columns]\n",
      "\n",
      "train: (533, 14) val: (178, 14) test: (178, 14)\n"
     ]
    }
   ],
   "source": [
    "# Applying the splitting function to the training set to get a validation set\n",
    "train, validation = train_test_split(train, test_size=0.25)\n",
    "\n",
    "if validation.shape[0] == test.shape[0]:  # Checking that the validation and testing set have the same number of rows\n",
    "  print(\"The validation and testing sets have the same number of rows!\")\n",
    "  print(f\"The new training set is:\\n{train}\\n\")\n",
    "  print(f\"The validation set is:\\n{validation}\\n\")\n",
    "else:\n",
    "  raise Exception(\"Sorry, the training and testing sets don't have the same number of rows\")\n",
    "print(\"train:\", train.shape,\"val:\", validation.shape, \"test:\", test.shape, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8-pkNzTNOm0n"
   },
   "source": [
    "Splitting the dataset in training, validation and testing set isn't enough: in fact, before feeding such data to a neural network, the target variable should be given in input to the network *separately* from the train, validation and test data. This has to be done in order to correctly train, validate and test the network because if the target variable isn't separated from the other features of the data, the neural network will just output it directly, without learning anything\n",
    "\n",
    "Let's suppose we want to train a neural network to classify if a passenger given in input will survive or not. In this case, the target variable is Survived. Now separate the target variable from the other features of the training, validation and testing sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "id": "qYQAaf--Rcln"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features of the training set:\n",
      "(533, 13)\n",
      "Target variabke of the training set\n",
      ":(533,)\n",
      "\n",
      "Features of the training set:\n",
      "(178, 13)\n",
      "Target variabke of the training set\n",
      ":(178,)\n",
      "\n",
      "Features of the training set:\n",
      "(178, 13)\n",
      "Target variabke of the training set\n",
      ":(178,)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_training = train[train.columns.difference(['Survived'])]# Selecting all the features of the training set but the column Survived\n",
    "y_training = train['Survived']\n",
    "print(f\"Features of the training set:\\n{x_training.shape}\\nTarget variabke of the training set\\n:{y_training.shape}\\n\")\n",
    "\n",
    "x_validation = validation[validation.columns.difference(['Survived'])]\n",
    "y_validation = validation['Survived']\n",
    "print(f\"Features of the training set:\\n{x_validation.shape}\\nTarget variabke of the training set\\n:{y_validation.shape}\\n\")\n",
    "\n",
    "x_testing = test[test.columns.difference(['Survived'])]\n",
    "y_testing = test['Survived']\n",
    "print(f\"Features of the training set:\\n{x_testing.shape}\\nTarget variabke of the training set\\n:{y_testing.shape}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rCFNGN0KTdNH"
   },
   "source": [
    "# Feature scaling\n",
    "The scaling of the features of the dataset is typically the last step of the preprocessing of a dataset. Since the range of values of the features of a dataset usually varies widely, scaling is done in order to obtain a uniform scale of values between all the features of the dataset; doing so typically facilitates the training phase of a machine learning algorithm.\n",
    "\n",
    "Two approaches are typically used in order to scale the data of a dataset: **normalization** and **standardization**. With normalization the values of the features are scaled in order to have values between 0 and 1, while standardization transforms the data of a feature to have a mean of zero and a standard deviation of 1.\n",
    "\n",
    "The *standard scaler* is among the most used methods of feature scaling through standardization. The standard scaler acts on a feature by removing the mean and scaling to unit variance. The scaled value *z* of a sample *x* of a feature is calculated as:\n",
    "\n",
    "*z = (x - u) / s*\n",
    "\n",
    "where *u* is the mean of the samples in the feature, and *s* is the standard deviation of the samples of the feature.\n",
    "\n",
    "Now, let's try using the standard scaler on the feature Fare of the training, validation and testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Fare\n",
      "504  108.9000\n",
      "252   16.1000\n",
      "665   13.0000\n",
      "514   34.0208\n",
      "487    8.0500\n",
      "..        ...\n",
      "568    7.8542\n",
      "790   26.0000\n",
      "77    29.0000\n",
      "386   13.0000\n",
      "831    7.8542\n",
      "\n",
      "[533 rows x 1 columns]\n",
      "(533, 13) (533,)\n"
     ]
    }
   ],
   "source": [
    "x = x_training[[\"Fare\"]]\n",
    "print(x)\n",
    "\n",
    "print(x_training.shape, y_training.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "id": "PwIKkHaHTn_G"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 values of the feature Fare in the training set before standard scaling are:\n",
      "         Fare\n",
      "504  108.9000\n",
      "252   16.1000\n",
      "665   13.0000\n",
      "514   34.0208\n",
      "487    8.0500\n",
      "720    7.0542\n",
      "317  164.8667\n",
      "6     51.8625\n",
      "31   146.5208\n",
      "98    26.0000\n",
      "after they are:\n",
      "[[ 1.46570951]\n",
      " [-0.33750122]\n",
      " [-0.39773779]\n",
      " [ 0.01072053]\n",
      " [-0.49392198]\n",
      " [-0.51327152]\n",
      " [ 2.55320685]\n",
      " [ 0.35740528]\n",
      " [ 2.19672492]\n",
      " [-0.14513284]]\n",
      "\n",
      "\n",
      "First 10 values of the feature Fare in the training set before standard scaling are:\n",
      "         Fare\n",
      "885   30.0000\n",
      "863   13.0000\n",
      "100    7.8958\n",
      "380   15.7417\n",
      "225   10.5000\n",
      "392  113.2750\n",
      "788   79.2000\n",
      "376  211.5000\n",
      "12     8.0500\n",
      "182   39.0000\n",
      "after they are:\n",
      "[[-0.06740824]\n",
      " [-0.39773779]\n",
      " [-0.49691827]\n",
      " [-0.34446341]\n",
      " [-0.44631567]\n",
      " [ 1.55072079]\n",
      " [ 0.88860435]\n",
      " [ 3.45934551]\n",
      " [-0.49392198]\n",
      " [ 0.10747211]]\n",
      "\n",
      "\n",
      "First 10 values of the feature Fare in the training set before standard scaling are:\n",
      "        Fare\n",
      "406  18.7500\n",
      "168  56.4958\n",
      "620  52.5542\n",
      "509   7.7500\n",
      "681   9.2250\n",
      "291  12.8750\n",
      "82   47.1000\n",
      "798  24.1500\n",
      "416  13.0000\n",
      "175  25.4667\n",
      "after they are:\n",
      "[[-0.28600868]\n",
      " [ 0.44743563]\n",
      " [ 0.37084581]\n",
      " [-0.49975133]\n",
      " [-0.47109038]\n",
      " [-0.40016668]\n",
      " [ 0.26486443]\n",
      " [-0.18108047]\n",
      " [-0.39773779]\n",
      " [-0.15549547]]\n"
     ]
    }
   ],
   "source": [
    "data_to_scale_training = x_training[[\"Fare\"]]  # Dataframe with the values of Fare in the training set\n",
    "data_to_scale_validation = x_validation[[\"Fare\"]]  # Dataframe with the values of Fare in the validation set\n",
    "data_to_scale_testing =   x_testing[[\"Fare\"]]# Dataframe with the values of Fare in the testing set\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler  # Importing the scaler\n",
    "standard_scaler = StandardScaler()\n",
    "\n",
    "X_train_std = standard_scaler.fit_transform(data_to_scale_training)  # Fitting the scaler on the training examples and transforming the values \n",
    "# IMPORTANT! The scaler must be fit on the training set data ONLY\n",
    "X_val_std = standard_scaler.transform(data_to_scale_validation)\n",
    "X_test_std = standard_scaler.transform(data_to_scale_testing)\n",
    "\n",
    "\n",
    "print(f\"First 10 values of the feature Fare in the training set before standard scaling are:\\n{data_to_scale_training.head(10)}\\nafter they are:\\n{X_train_std[:10]}\\n\\n\")\n",
    "print(f\"First 10 values of the feature Fare in the training set before standard scaling are:\\n{data_to_scale_validation.head(10)}\\nafter they are:\\n{X_val_std[:10]}\\n\\n\")\n",
    "print(f\"First 10 values of the feature Fare in the training set before standard scaling are:\\n{data_to_scale_testing.head(10)}\\nafter they are:\\n{X_test_std[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QxidL81qan2x"
   },
   "source": [
    "The *minmax scaler* is among the most used methods of feature scaling through normalization. The minmax scaler acts by setting the scaled value of the maximal non-scaled value of a feature to 1 and the scaled value of the minimal non-scaled value of the feature to 0. The values of the other entries of the feature are then scaled accordingly to the new range of values.\n",
    "\n",
    "Now, let's try using the minmax scaler on the feature Fare of the training, validation and testing dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "id": "ky9lb6RkZb4T"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 values of the feature Fare in the training set before minmax scaling are:\n",
      "         Fare\n",
      "504  108.9000\n",
      "252   16.1000\n",
      "665   13.0000\n",
      "514   34.0208\n",
      "487    8.0500\n",
      "720    7.0542\n",
      "317  164.8667\n",
      "6     51.8625\n",
      "31   146.5208\n",
      "98    26.0000\n",
      "after they are:\n",
      "[[0.21255864]\n",
      " [0.03142511]\n",
      " [0.02537431]\n",
      " [0.06640418]\n",
      " [0.01571255]\n",
      " [0.01376888]\n",
      " [0.32179837]\n",
      " [0.10122886]\n",
      " [0.28598956]\n",
      " [0.05074862]]\n",
      "\n",
      "\n",
      "First 10 values of the feature Fare in the training set before minmax scaling are:\n",
      "         Fare\n",
      "885   30.0000\n",
      "863   13.0000\n",
      "100    7.8958\n",
      "380   15.7417\n",
      "225   10.5000\n",
      "392  113.2750\n",
      "788   79.2000\n",
      "376  211.5000\n",
      "12     8.0500\n",
      "182   39.0000\n",
      "after they are:\n",
      "[[0.0585561 ]\n",
      " [0.02537431]\n",
      " [0.01541158]\n",
      " [0.03072575]\n",
      " [0.02049464]\n",
      " [0.22109808]\n",
      " [0.1545881 ]\n",
      " [0.41282051]\n",
      " [0.01571255]\n",
      " [0.07612293]]\n",
      "\n",
      "\n",
      "First 10 values of the feature Fare in the training set before minmax scaling are:\n",
      "        Fare\n",
      "406  18.7500\n",
      "168  56.4958\n",
      "620  52.5542\n",
      "509   7.7500\n",
      "681   9.2250\n",
      "291  12.8750\n",
      "82   47.1000\n",
      "798  24.1500\n",
      "416  13.0000\n",
      "175  25.4667\n",
      "after they are:\n",
      "[[0.03659756]\n",
      " [0.11027246]\n",
      " [0.10257897]\n",
      " [0.01512699]\n",
      " [0.018006  ]\n",
      " [0.02513033]\n",
      " [0.09193308]\n",
      " [0.04713766]\n",
      " [0.02537431]\n",
      " [0.04970769]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler  # Importing the scaler\n",
    "minmax_scaler = MinMaxScaler()\n",
    "\n",
    "X_train_norm = minmax_scaler.fit_transform(data_to_scale_training)\n",
    "X_val_norm = minmax_scaler.transform(data_to_scale_validation)\n",
    "X_test_norm = minmax_scaler.transform(data_to_scale_testing)\n",
    "\n",
    "print(f\"First 10 values of the feature Fare in the training set before minmax scaling are:\\n{data_to_scale_training.head(10)}\\nafter they are:\\n{X_train_norm[:10]}\\n\\n\")\n",
    "print(f\"First 10 values of the feature Fare in the training set before minmax scaling are:\\n{data_to_scale_validation.head(10)}\\nafter they are:\\n{X_val_norm[:10]}\\n\\n\")\n",
    "print(f\"First 10 values of the feature Fare in the training set before minmax scaling are:\\n{data_to_scale_testing.head(10)}\\nafter they are:\\n{X_test_norm[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNROUPrVJu8CZXKQasiXbja",
   "collapsed_sections": [],
   "name": "03_Data_Preprocessing.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
